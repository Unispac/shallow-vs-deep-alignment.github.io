<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Safety Alignment Should Be Made More Than Just a Few Tokens Deep">
  <meta name="keywords" content="LLM, Safety Alignment, Adversarial Attacks">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Safety Alignment Should Be Made More Than Just a Few Tokens Deep</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <link rel="stylesheet" href="https://code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  <script src="https://code.jquery.com/ui/1.12.1/jquery-ui.min.js"></script>







  

  <style>
      body {
          overflow-x: hidden; /* Hide horizontal overflow */
      }
  </style>



  <style>
    .blue-text {
      color: blue;
    }
  </style>

  <style>
    .red-text {
      color: red;
    }
  </style>


<style>
  .video-wrapper {
      position: relative;
      overflow: hidden;
      width: 100%;
      padding-top: 56.25%; /* For 16:9 aspect ratio */
  }

  .video-frame {
      position: absolute;
      top: 0;
      left: 0;
      bottom: 0;
      right: 0;
      width: 100%;
      height: 100%;
      border: none;
  }

</style>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Safety Alignment Should Be Made More Than Just a Few Tokens Deep</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://unispac.github.io/">Xiangyu Qi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://kiddyboots216.github.io/">Ashwinee Panda</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://kaifeng.ac/">Kaifeng Lyu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://maxiao.info/">Xiao Ma</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.in/citations?user=CZhNxjgAAAAJ&hl=en">Subhrajit Roy</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/beirami/home">Ahmad Beirami</a><sup>2</sup>,</span><br>
            <span class="author-block">
              <a href="https://www.princeton.edu/~pmittal/">Prateek Mittal</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.peterhenderson.co/">Peter Henderson</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Princeton University,</span>
            <span class="author-block"><sup>2</sup>Google DeepMind</span>
          </div>

          <!-- <div class="column has-text-centered">
            <div class="publication-links"> -->
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->

          <!-- </div> -->
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. We also design a regularized fine-tuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.
          </p>
        </div>
      </div>
    </div>

</section>



<footer class="footer" style="background-color: white;">
  <div class="container">
      <p class="text-muted">
          Acknowledgements: The website template was borrowed from
          <a href="https://llm-tuning-safety.github.io/"target="_blank">LLM Finetuning Risks</a>,
          <a href="https://climatenerf.github.io/" target="_blank">ClimateNeRF</a>, 
          <a href="http://mgharbi.com/" target="_blank">MichaÃ«l Gharbi</a>, 
          <a href="https://dorverbin.github.io/refnerf/index.html" target="_blank">RefNeRF</a>, 
          <a href="https://nerfies.github.io/" target="_blank">Nerfies</a> 
          and <a href="https://hhsinping.github.io/svs/supp/visual_comparison.html" target="_blank">Semantic View Synthesis</a>.
      </p>
  </div>
</footer>

</body>
</html>
